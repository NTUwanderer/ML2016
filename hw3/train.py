import pickle, json, argparse
import numpy as np

# from keras.models import model_from_json
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
# from keras.models import load_model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras.utils import np_utils

parser = argparse.ArgumentParser()
parser.add_argument('path', help = 'path to data')
parser.add_argument('model_path', help = 'path to model')

args = parser.parse_args()

path = args.path
model_path = args.model_path

f = open(path + '/all_label.p', 'rb')

batch_size = 32
nb_classes = 10
nb_epoch = 30

all_label = pickle.load(f)
f.close()

data = []
answer = np.zeros((5000, 10), dtype=np.float)

index = 0
for index_array in all_label:
    for picture in index_array[0:500]:
        data.append(np.array(picture, dtype=np.float).reshape(3,32,32))

    for j in range(500 * index, 500 * (index + 1)):
        answer[j][index] = 1.0

    index += 1

data = np.array(data)

model = Sequential()
model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=data.shape[1:]))
model.add(Activation('relu'))
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Convolution2D(64, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

data = data.astype('float32')
data /= 255

# model.fit(data, answer, batch_size=batch_size, nb_epoch=nb_epoch, shuffle=True)

# this will do preprocessing and realtime data augmentation
datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(data)

# fit the model on the batches generated by datagen.flow()
model.fit_generator(datagen.flow(data, answer,
                    batch_size=batch_size),
                    samples_per_epoch=data.shape[0],
                    nb_epoch=nb_epoch)

model.save(model_path)

